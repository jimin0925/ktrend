import os
import asyncio
import json
from openai import AsyncOpenAI
from dotenv import load_dotenv
from backend.database import Database

# Load env to get OPENAI_API_KEY
load_dotenv()

class TrendAnalyzer:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        if self.api_key:
            self.client = AsyncOpenAI(api_key=self.api_key)
        else:
            self.client = None
            print("[WARNING] No OPENAI_API_KEY found. Analyzer will return mock data.")
            
        # Database
        self.db = Database()

    async def analyze_trend(self, keyword: str):
        """
        Analyze trend using OpenAI's Native Web Search (Implicit) + Naver Datalab Data.
        Returns:
            dict: { "keyword": str, "reason": str, "chart_data": list }
        """
        if not self.client:
            return self._get_mock_analysis(keyword)

        # 1. Fetch Naver Datalab Data (Async wrap)
        from backend.services.naver_datalab import NaverDataLab
        try:
            datalab_service = NaverDataLab()
            chart_data = await asyncio.to_thread(datalab_service.get_daily_trend, keyword, days=365)
            
            # Create a summary for LLM context
            if chart_data:
                peak = max(chart_data, key=lambda x: x['ratio'])
                recent = chart_data[-1] if chart_data else None
                data_context = f"ë„¤ì´ë²„ ê²€ìƒ‰ëŸ‰ ì¶”ì´ (1ë…„): {chart_data[0]['date']}~{chart_data[-1]['date']}. ìµœê³ ì : {peak['date']} ({peak['ratio']}). ìµœê·¼: {recent['date']} ({recent['ratio']})."
            else:
                data_context = "ë„¤ì´ë²„ ê²€ìƒ‰ëŸ‰ ë°ì´í„° ì—†ìŒ."
        except Exception as e:
            print(f"Naver DataLab Error: {e}")
            chart_data = []
            data_context = "ë„¤ì´ë²„ ê²€ìƒ‰ëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨."

        # 2. Check Database for Cached Reason
        cached_analysis = self.db.get_analysis(keyword)
        if cached_analysis:
            print(f"[DB HIT] Returning stored analysis for '{keyword}'")
            return {
                "keyword": keyword,
                "reason": cached_analysis["reason"],
                "chart_data": chart_data # Always return fresh chart data + cached reason
            }

        print(f"Analyzing trend for: {keyword} (LLM Call)...")
        
        # 3. LLM Analysis
        system_prompt = """
        ë„ˆëŠ” í•œêµ­ì˜ ìµœì‹  íŠ¸ë Œë“œë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
        **ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥**ì„ ì‚¬ìš©í•˜ì—¬ ì´ í‚¤ì›Œë“œê°€ **ì™œ** ìœ í–‰í•˜ëŠ”ì§€ ì •í™•í•œ 'ìœ ë˜'ì™€ 'ì´ìœ 'ë¥¼ ì°¾ì•„ë‚´.

        **ë¶„ì„ ê°€ì´ë“œë¼ì¸ (ì¤‘ìš”)**:
        í‚¤ì›Œë“œë¥¼ ë‹¤ìŒ 3ê°€ì§€ ìœ í˜• ì¤‘ í•˜ë‚˜ë¡œ íŒë‹¨í•˜ì—¬ ë¶„ì„í•´:

        1. **ğŸ”¥ ë°”ì´ëŸ´ íŠ¸ë Œë“œ (Viral Trend) & íŒŒìƒ ìœ í–‰**:
           - ìµœê·¼ ë‰´ìŠ¤, ë°ˆ, ë°©ì†¡, SNS ë“±ì—ì„œ í™”ì œê°€ ë˜ì–´ ê¸‰ìƒìŠ¹í•œ ê²½ìš°.
           - **[ì¤‘ìš”] ì‹ì¬ë£Œ/ë¶€ì†í’ˆì¼ ê²½ìš°**: ì´ ì¬ë£Œê°€ ë“¤ì–´ê°€ëŠ” **ìƒìœ„ ìœ í–‰(Parent Trend)**ì´ ìˆëŠ”ì§€ ë°˜ë“œì‹œ í™•ì¸í•´.
             - ì˜ˆ: 'í”¼ìŠ¤íƒ€ì¹˜ì˜¤ ìŠ¤í”„ë ˆë“œ' -> 'ë‘ë°”ì´ ì´ˆì½œë¦¿' ìœ í–‰ ë•Œë¬¸ì„.
           - **[ì£¼ì˜] ê³¼ê±° ì´ìŠˆ ë°°ì œ**: ë§Œì•½ 'ë‹¤ì´ì–´íŠ¸', 'ì—°ì˜ˆì¸' ê´€ë ¨ ë‰´ìŠ¤ë¼ë©´, ê·¸ê²ƒì´ **"í˜„ì¬(ìµœê·¼ 1ê°œì›” ë‚´)"** ì´ìŠˆì¸ì§€ í™•ì¸í•´.
             - ì˜ˆ: 'ìŒ€ 20kg' -> í™ìœ¤í™” ë‹¤ì´ì–´íŠ¸(ê³¼ê±°) ì–¸ê¸‰ ê¸ˆì§€.

        2. **ğŸ›’ ìƒí•„í’ˆ/ìŠ¤í…Œë””ì…€ëŸ¬ (Steady Seller)**:
           - **[ìµœìš°ì„ ]** ìœ í–‰/ê³„ì ˆê³¼ ìƒê´€ì—†ì´ 1ë…„ ë‚´ë‚´ ë¨¹ê±°ë‚˜ ì“°ëŠ” **í•„ìˆ˜ì¬**.
           - ì˜ˆì‹œ: **ìŒ€(Rice)**, ìƒìˆ˜, ê¹€ì¹˜, í™”ì¥ì§€, ë¬¼í‹°ìŠˆ, ë¼ë©´ ë“±.
           - ë¶„ì„: "í•œêµ­ì¸ì˜ ì£¼ì‹ì´ì ì¼ìƒì ìœ¼ë¡œ ì†Œë¹„ë˜ëŠ” í•„ìˆ˜í’ˆì…ë‹ˆë‹¤. ìµœê·¼ ë¬¼ê°€/ì¬êµ¬ë§¤ ì£¼ê¸°ì— ë”°ë¼ ê²€ìƒ‰ëŸ‰ì´ ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¼ê³  ì„¤ëª….

        3. **ğŸ‚ ê³„ì ˆ/ë‚ ì”¨ í•„ìˆ˜ê°€ (Seasonal Necessity)**:
           - íŠ¹ì • **ë‚ ì”¨/ì‹œì¦Œ(ëª…ì ˆ, íœ´ê°€)**ì—ë§Œ ìˆ˜ìš”ê°€ í­ë°œí•˜ëŠ” ê²½ìš°.
           - ì˜ˆì‹œ: ë¡±íŒ¨ë”©(ê²¨ìš¸), ì„ í’ê¸°(ì—¬ë¦„), ë ˆì¸ë¶€ì¸ (ì¥ë§ˆ), ì„ ë¬¼ì„¸íŠ¸(ëª…ì ˆ).
           - **ì£¼ì˜**: 'ìŒ€'ì€ ê°€ì„(í–…ìŒ€) ì´ìŠˆê°€ ìˆì–´ë„ ê¸°ë³¸ì ìœ¼ë¡œ **2ë²ˆ(ìƒí•„í’ˆ)**ìœ¼ë¡œ ë¶„ë¥˜í•´.

        **í• ë£¨ì‹œë„¤ì´ì…˜ ë° ë§í¬ ë°©ì§€**:
        - **ì ˆëŒ€ ë§í¬ë¥¼ í¬í•¨í•˜ì§€ ë§ˆ.** (http, www, .com ë“± ê¸ˆì§€)
        - ì¶œì²˜ í‘œê¸° ê¸ˆì§€: "(news.nate.com)", "[ë„¤ì´ë²„ ë‰´ìŠ¤]" ê°™ì€ í…ìŠ¤íŠ¸ ì ˆëŒ€ ì“°ì§€ ë§ˆ.
        
        **ì‘ë‹µ ìŠ¤íƒ€ì¼ (ë§¤ìš° ì¤‘ìš”)**:
        - **ë³¼ë“œì²´(**)**, **[ë§í¬]** ë“± ëª¨ë“  ì„œì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆ.** (ìˆœìˆ˜í•œ ì¤„ê¸€ í…ìŠ¤íŠ¸ë§Œ)
        - "ë¶„ì„ ê²°ê³¼: ..." ê°™ì€ ì„œë‘ ì—†ì´ ë°”ë¡œ ë³¸ë¡ ë§Œ ì‘ì„±í•´.
        - **[ìˆ˜ì¹˜ ì–¸ê¸‰ ê¸ˆì§€]**: "100ì„ ê¸°ë¡í–ˆë‹¤", "ratio 90" ê¸ˆì§€. "ê¸‰ì¦í–ˆë‹¤", "ë†’ì€ ì¸ê¸°ë¥¼ ìœ ì§€í•˜ê³  ìˆë‹¤" í‘œí˜„ ì‚¬ìš©.
        """

        user_prompt = f"""
        í‚¤ì›Œë“œ: '{keyword}'ì˜ ì •í™•í•œ í•œêµ­ ë‚´ ìœ í–‰ ì´ìœ ì™€ ìœ ë˜ë¥¼ ì›¹ ê²€ìƒ‰ì„ í†µí•´ ë¶„ì„í•´ì¤˜.
        
        [ì°¸ê³  ë°ì´í„°]
        {data_context}
        """
        
        try:
            # Native Web Search - Implicit (JSON mode not supported with search)
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini-search-preview", 
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            content = response.choices[0].message.content
            
            # Post-processing: Remove Markdown formatting (links, bold)
            import re
            # Remove links [text](url) -> text
            content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
            # Remove bare URLs
            content = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', content)
            # Remove domain citations like (news.nate.com) or (www.naver.com)
            content = re.sub(r'\([a-zA-Z0-9.-]+\.[a-z]{2,}\)', '', content)
            # Remove bold **text** -> text
            content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)
            # Remove bold __text__ -> text
            content = re.sub(r'__([^_]+)__', r'\1', content)
            
            # Save to DB
            self.db.save_analysis(keyword, content, chart_data)
            
            return {
                "keyword": keyword,
                "reason": content,
                "chart_data": chart_data
            }
            
        except Exception as e:
            print(f"LLM/Search error: {e}")
            return self._get_mock_analysis(keyword)

    def _get_mock_analysis(self, keyword):
        return {
            "keyword": keyword,
            "reason": "AI ë¶„ì„ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì„ì‹œ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
            "chart_data": []
        }

